# -*- coding: utf-8 -*-
"""MLE_FINALPROJECT_NN_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18v9jguXIuNByEoJ2QF6FOXScRfUn_6Ke

## Data Preprocessing
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
# %matplotlib inline

# Ignore Warnings
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

#Write out the versions of all packages to requirements.txt
!pip freeze >> requirements.txt

# Remove the restriction on Jupyter that limits the columns displayed (the ... in the middle)
pd.set_option("display.max_columns", None)
pd.set_option('display.max_rows', None)
# Docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.set_option.html#

# Pretty Display of variables.  for instance, you can call df.head() and df.tail() in the same cell and BOTH display w/o print
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

# to display nice model diagram
from sklearn import set_config
set_config(display='diagram')

# Python ≥3.5 is required
import sys
assert sys.version_info >= (3, 5)

# Scikit-Learn ≥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"

print("\n Numpy: " + np.__version__)
print("\n sklearn: " + sklearn.__version__)

"""### Structural Analysis"""

df_complete = pd.read_csv('https://raw.githubusercontent.com/mounicakakarla1211/MLETraining/main/chasehmda.csv',delimiter="|")
df_complete.head()

selected_columns = ['loan_amount','income','combined_loan_to_value_ratio','loan_term',
           'loan_type', 'loan_purpose','construction_method', 'occupancy_type',
           'purchaser_type','applicant_credit_scoring_model','co_applicant_credit_scoring_model',
           'debt_to_income_ratio','business_or_commercial_purpose','action_taken',
           'applicant_age','applicant_sex','co_applicant_sex','applicant_ethnicity_1','co_applicant_ethnicity_1','applicant_race_1','co_applicant_race_1','state_code']
df = df_complete[selected_columns]
df.sample(5)

print("Rows and Columns: \n", df.shape, "\n")
print("General Information: \n", df.info(), "\n")

# Count how many times each data type is present in the dataset
pd.value_counts(df.dtypes)
# How many unique values per feature
df.nunique().to_frame()

# Pull descriptive statistics from your overall dataset
df.describe().T

"""#### Change data types to the correct data types"""

df["loan_type"]=df["loan_type"].apply(str)
df["loan_purpose"]=df["loan_purpose"].apply(str)
df["construction_method"]=df["construction_method"].apply(str)
df["occupancy_type"]=df["occupancy_type"].apply(str)
df["purchaser_type"]=df["purchaser_type"].apply(str)
df["applicant_credit_scoring_model"]=df["applicant_credit_scoring_model"].apply(str)
df["co_applicant_credit_scoring_model"]=df["co_applicant_credit_scoring_model"].apply(str)
df["debt_to_income_ratio"]=df["debt_to_income_ratio"].apply(str)
df["business_or_commercial_purpose"]=df["business_or_commercial_purpose"].apply(str)
df["applicant_age"]=df["applicant_age"].apply(str)
df["applicant_sex"]=df["applicant_sex"].apply(str)
df["co_applicant_sex"]=df["co_applicant_sex"].apply(str)
df["applicant_ethnicity_1"] = df["applicant_ethnicity_1"].apply(str)
df["co_applicant_ethnicity_1"] = df["co_applicant_ethnicity_1"].apply(str)
df["applicant_race_1"] = df["applicant_race_1"].apply(str)
df["co_applicant_race_1"] = df["co_applicant_race_1"].apply(str)
df["action_taken"]=df["action_taken"].apply(str)

df.shape

"""#### converting target action_taken to binary classfication
- 1 -- Loan originated
- 2 -- Application approved but not accepted
- 3 -- Application denied
- 4 -- Application withdrawn by applicant
- 5 -- File closed for incompleteness
- 6 -- Purchased loan
- 7 -- Preapproval request denied
- 8 -- Preapproval request approved but not accepted
1,2,6,8 - approved 1 remaining unapproved 0

"""

df['action_taken'].unique()

d = {'1': 1,'2':1,'6':1,'8':1}
df['approved'] = df['action_taken'].map(d).fillna(0)

df['approved'].unique()

df.drop(columns=['action_taken'],inplace=True)

"""### Quality Analysis

#### Duplicates
"""

# Duplicates in the Columns?
df.duplicated().sum()
#drop duplicates
df.drop_duplicates(keep='first',inplace=True)

"""#### MISSING VALUES"""

# MISSING VALUES
df.isna().sum()

plt.figure(figsize=(15, 8))
sns.set_style('whitegrid')

g = sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
g.set_xlabel('Column Number')
g.set_ylabel('Sample Number')

# Missing Values per Feature (Big Holes)
df.isna().mean().sort_values().plot(
    kind="bar", figsize=(15, 4),
    title="Percentage of missing values per feature",
    ylabel="Ratio of missing values per feature");

# drop any col that is more than 80% empty
df = df.dropna(thresh=df.shape[0] * 0.20,axis=1)
df.shape

# Make a decision... drop rows that are 20% or more empty (you set the threshhold)
df = df.dropna(thresh=df.shape[1] * 0.80, axis = 0).reset_index(drop=True)
df.shape

plt.figure(figsize=(15, 8))
sns.set_style('whitegrid')

g = sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
# g = sns.heatmap(df_X.loc[df_X.isnull().sum(1).sort_values(ascending=1).index].isnull(), cbar=False, cmap='viridis')
g.set_xlabel('Column Number')
g.set_ylabel('Sample Number')

df.isna().sum()

numerics = ['int16','int32','int64','float64']

df.select_dtypes(exclude=numerics).columns

df.select_dtypes(include=numerics).columns

# filling na values
df[df.select_dtypes(include=numerics).columns] = df.select_dtypes(include=numerics).fillna(0)
df[df.select_dtypes(exclude=numerics).columns] = df.select_dtypes(exclude=numerics).fillna('Missing')
df[df.select_dtypes(exclude=numerics).columns] = df.select_dtypes(exclude=numerics).replace('nan',"Missing")

df.drop(df[df['state_code'] == 'Missing'].index, inplace = True)

"""#### Outliers"""

df = df[df['income'] != 0.0]
df['loanamount_log'] = np.log(df['loan_amount'])
df['income_log'] = np.log(df['income'])

df.drop(columns=['loan_amount','income'],inplace=True)

"""### Cleaning Data"""

# Basic Data Cleaning
df.columns = df.columns.str.lower().str.replace(' ', '_') # A

string_columns = list(df.dtypes[df.dtypes == 'object'].index) # B

for col in string_columns:
    df[col] = df[col].str.lower().str.replace('<', 'less') # C
    df[col] = df[col].str.lower().str.replace('>', 'greater') # C

df.info()

"""## Data Preparation for Modeling

### Standardization and Normalization
"""

from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()

numerics = ['int16','int32','int64','float64']
df_sample = df.sample(100000)
catDF = df_sample.select_dtypes(exclude=numerics)
numDF = df_sample.select_dtypes(include=numerics)

numDF.head()
numDF.drop(columns='approved',inplace=True)

scaler.fit_transform(numDF.values)# Scale all numeric columns
numDF = pd.DataFrame(scaler.fit_transform(numDF.values),columns=numDF.columns,index=numDF.index)
numDF.head()

"""### Encoding"""

# Encode your cat data
catDF = pd.get_dummies(catDF,drop_first=True)

print(catDF.columns)
np.save("columns.txt",catDF.columns)

catDF.head()

X = pd.concat([catDF, numDF],axis=1)
print(X.shape)
#Prepare the Y variable
Y = df_sample['approved']

print(Y.shape)
X.head()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=123)

X_train.shape

y_train.shape

"""## Neural Networks"""

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import backend as K

tf.random.set_seed(42)


model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(
    loss=tf.keras.losses.binary_crossentropy,
    optimizer=tf.keras.optimizers.Adam(lr=0.03),
    metrics=[
        tf.keras.metrics.BinaryAccuracy(name='accuracy'),
        tf.keras.metrics.Precision(name='precision'),
        tf.keras.metrics.Recall(name='recall')
    ]
)
model.fit(X_train, y_train, epochs=100)

predictions = model.predict(X_test)
predictions[:5]

predictions_a = np.ravel(predictions)
predictions_a[:5]

prediction_classes = [
    1 if prob > 0.5 else 0 for prob in np.ravel(predictions)
]

from sklearn.metrics import confusion_matrix

print(confusion_matrix(y_test, prediction_classes))

from sklearn.metrics import accuracy_score, precision_score, recall_score


print(f'Accuracy: {accuracy_score(y_test, prediction_classes):.2f}')
print(f'Precision: {precision_score(y_test, prediction_classes):.2f}')
print(f'Recall: {recall_score(y_test, prediction_classes):.2f}')

from tensorflow.keras.utils import plot_model

# summarize the model
plot_model(model, 'model.png', show_shapes=True)

from sklearn.metrics import classification_report
print(classification_report(y_test, prediction_classes))

model_columns = pd.DataFrame(X.columns, columns = ['Features'])
model_columns.to_csv('model_columns.csv', index = False)

"""## Resampling"""

trainData = pd.concat([X_train,y_train],axis=1)

ind = trainData[trainData['approved']==0].index
print(len(ind))

# Separate the minority class
minData = trainData.loc[ind]
print(minData.shape)

# now the majority
ind1 = trainData[trainData['approved']==1].index
print(len(ind1))

# Separate the majority class
majData = trainData.loc[ind1]
print(majData.shape)
majData.head()

majSample=majData.sample(n=len(ind),random_state=123)

print(majSample.shape)
majSample.head()

# Concatenating both data sets and then shuffling the data set
balData = pd.concat([minData,majSample],axis = 0)

# Shuffling the data set
from sklearn.utils import shuffle
balData = shuffle(balData)
balData.head()

X_train_bal = balData
y_train_bal = balData['approved']
del X_train_bal['approved']

model.fit(X_train_bal, y_train_bal, epochs=100)

predictions = model.predict(X_test)
predictions[:5]

prediction_classes = [
    1 if prob > 0.5 else 0 for prob in np.ravel(predictions)
]

from sklearn.metrics import classification_report
print(classification_report(y_test, prediction_classes))

import pickle
file_name = "nn_reg.pkl"
# save
pickle.dump(model, open(file_name, "wb"))

