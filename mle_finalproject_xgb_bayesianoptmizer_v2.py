# -*- coding: utf-8 -*-
"""MLE_FINALPROJECT_XGB_bayesianoptmizer_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1do8XTWnUiuw1eEAa6D3uiubKiUpK0Nbl

## Data Preprocessing
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
# %matplotlib inline

# Ignore Warnings
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

#Write out the versions of all packages to requirements.txt
!pip freeze >> requirements.txt

# Remove the restriction on Jupyter that limits the columns displayed (the ... in the middle)
pd.set_option("display.max_columns", None)
pd.set_option('display.max_rows', None)
# Docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.set_option.html#

# Pretty Display of variables.  for instance, you can call df.head() and df.tail() in the same cell and BOTH display w/o print
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

# to display nice model diagram
from sklearn import set_config
set_config(display='diagram')

# Python ≥3.5 is required
import sys
assert sys.version_info >= (3, 5)

# Scikit-Learn ≥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"

print("\n Numpy: " + np.__version__)
print("\n sklearn: " + sklearn.__version__)

"""### Structural Analysis"""

df_complete = pd.read_csv('https://raw.githubusercontent.com/mounicakakarla1211/MLETraining/main/chasehmda.csv',delimiter="|")
df_complete.head()

df_complete.columns

selected_columns = ['loan_amount','income','combined_loan_to_value_ratio','loan_term',
           'loan_type', 'loan_purpose','construction_method', 'occupancy_type',
           'purchaser_type','applicant_credit_scoring_model','co_applicant_credit_scoring_model',
           'debt_to_income_ratio','business_or_commercial_purpose','action_taken',
           'applicant_age','applicant_sex','co_applicant_sex','applicant_ethnicity_1','co_applicant_ethnicity_1','applicant_race_1','co_applicant_race_1','state_code']
df = df_complete[selected_columns]
df.sample(5)

print("Rows and Columns: \n", df.shape, "\n")
print("General Information: \n", df.info(), "\n")

# Count how many times each data type is present in the dataset
pd.value_counts(df.dtypes)
# How many unique values per feature
df.nunique().to_frame()

# Pull descriptive statistics from your overall dataset
df.describe().T

"""#### Change data types to the correct data types"""

df["loan_type"]=df["loan_type"].apply(str)
df["loan_purpose"]=df["loan_purpose"].apply(str)
df["construction_method"]=df["construction_method"].apply(str)
df["occupancy_type"]=df["occupancy_type"].apply(str)
df["purchaser_type"]=df["purchaser_type"].apply(str)
df["applicant_credit_scoring_model"]=df["applicant_credit_scoring_model"].apply(str)
df["co_applicant_credit_scoring_model"]=df["co_applicant_credit_scoring_model"].apply(str)
df["debt_to_income_ratio"]=df["debt_to_income_ratio"].apply(str)
df["business_or_commercial_purpose"]=df["business_or_commercial_purpose"].apply(str)
df["applicant_age"]=df["applicant_age"].apply(str)
df["applicant_sex"]=df["applicant_sex"].apply(str)
df["co_applicant_sex"]=df["co_applicant_sex"].apply(str)
df["applicant_ethnicity_1"] = df["applicant_ethnicity_1"].apply(str)
df["co_applicant_ethnicity_1"] = df["co_applicant_ethnicity_1"].apply(str)
df["applicant_race_1"] = df["applicant_race_1"].apply(str)
df["co_applicant_race_1"] = df["co_applicant_race_1"].apply(str)
df["action_taken"]=df["action_taken"].apply(str)

df.shape

"""#### converting target action_taken to binary classfication
- 1 -- Loan originated
- 2 -- Application approved but not accepted
- 3 -- Application denied
- 4 -- Application withdrawn by applicant
- 5 -- File closed for incompleteness
- 6 -- Purchased loan
- 7 -- Preapproval request denied
- 8 -- Preapproval request approved but not accepted
1,2,6,8 - approved 1 remaining unapproved 0

"""

df['action_taken'].unique()

d = {'1': 1,'2':1,'6':1,'8':1}
df['approved'] = df['action_taken'].map(d).fillna(0)

df['approved'].unique()

df.drop(columns=['action_taken'],inplace=True)

"""### Quality Analysis

#### Duplicates
"""

# Duplicates in the Columns?
df.duplicated().sum()
#drop duplicates
df.drop_duplicates(keep='first',inplace=True)

"""#### MISSING VALUES"""

# MISSING VALUES
df.isna().sum()

plt.figure(figsize=(15, 8))
sns.set_style('whitegrid')

g = sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
g.set_xlabel('Column Number')
g.set_ylabel('Sample Number')

# Missing Values per Feature (Big Holes)
df.isna().mean().sort_values().plot(
    kind="bar", figsize=(15, 4),
    title="Percentage of missing values per feature",
    ylabel="Ratio of missing values per feature");

# drop any col that is more than 80% empty
df = df.dropna(thresh=df.shape[0] * 0.20,axis=1)
df.shape

# Make a decision... drop rows that are 20% or more empty (you set the threshhold)
df = df.dropna(thresh=df.shape[1] * 0.80, axis = 0).reset_index(drop=True)
df.shape

plt.figure(figsize=(15, 8))
sns.set_style('whitegrid')

g = sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
# g = sns.heatmap(df_X.loc[df_X.isnull().sum(1).sort_values(ascending=1).index].isnull(), cbar=False, cmap='viridis')
g.set_xlabel('Column Number')
g.set_ylabel('Sample Number')

df.isna().sum()

numerics = ['int16','int32','int64','float64']

df.select_dtypes(exclude=numerics).columns

df.select_dtypes(include=numerics).columns

# filling na values
df[df.select_dtypes(include=numerics).columns] = df.select_dtypes(include=numerics).fillna(0)
df[df.select_dtypes(exclude=numerics).columns] = df.select_dtypes(exclude=numerics).fillna('Missing')
df[df.select_dtypes(exclude=numerics).columns] = df.select_dtypes(exclude=numerics).replace('nan',"Missing")
df[df.select_dtypes(exclude=numerics).columns] = df.select_dtypes(exclude=numerics).replace('8888',"Missing")
df[df.select_dtypes(exclude=numerics).columns] = df.select_dtypes(exclude=numerics).replace('9999',"Missing")

df.drop(df[df['state_code'] == 'Missing'].index, inplace = True)

"""#### Outliers"""

df = df[df['income'] != 0.0]
df['loanamount_log'] = np.log(df['loan_amount'])
df['income_log'] = np.log(df['income'])

df.drop(columns=['loan_amount','income'],inplace=True)

df.describe().T

df.info()

"""### Cleaning Data"""

# Basic Data Cleaning
df.columns = df.columns.str.lower().str.replace(' ', '_') # A

string_columns = list(df.dtypes[df.dtypes == 'object'].index) # B

for col in string_columns:
    df[col] = df[col].str.lower().str.replace('<', 'less') # C
    df[col] = df[col].str.lower().str.replace('>', 'greater') # C

df = df.sample(50000)

"""## Data Preparation for Modeling

### Standardization and Normalization
"""

from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()

numerics = ['int16','int32','int64','float64']
catDF = df.select_dtypes(exclude=numerics)
numDF = df.select_dtypes(include=numerics)

numDF.head()
numDF.drop(columns='approved',inplace=True)

scaler.fit_transform(numDF.values)# Scale all numeric columns
numDF = pd.DataFrame(scaler.fit_transform(numDF.values),columns=numDF.columns,index=numDF.index)
numDF.head()

"""### Encoding"""

# Encode your cat data
catDF = pd.get_dummies(catDF,drop_first=True)

catDF.head()

X = pd.concat([catDF, numDF],axis=1)
print(X.shape)
#Prepare the Y variable
Y = df['approved']

print(Y.shape)
X.head()

print("General Information: \n", df.info(), "\n")

"""## XG Boost Model"""

from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=123)

X_train.shape

y_train.shape

hmdaModel = XGBClassifier(booster='gbtree')
hmdaModel.fit(X_train, y_train)

# This will give you the list of Hyperparameters of your model
hmdaModel.get_params()

hmdaModel.feature_importances_

# Very important.  Once you have a trained model - interegate the coefficients to see WHAT is important
feature_names=hmdaModel.get_booster().feature_names
# Evaluate the coefficients to learn what the model thinks is important in the predictions.
list_of_tuples = list(zip(feature_names, hmdaModel.feature_importances_))
df_imp = pd.DataFrame(list_of_tuples, columns = ['Feature', 'Importance'])
df_imp.sort_values(by='Importance', ascending=False)

df_imp[df_imp['Importance']> 0.008].sort_values(by='Importance').plot.barh(x='Feature')

from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import matplotlib.pyplot as plt
# predict_proba predicts the probability and predict just predicts the category
# y_pred = dt.predict_proba(X_test)[:, 1]
y_pred = hmdaModel.predict(X_test)

conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
#
# Print the confusion matrix using Matplotlib
#
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

"""## Resampling"""

trainData = pd.concat([X_train,y_train],axis=1)

ind = trainData[trainData['approved']==0].index
print(len(ind))

# Separate the minority class
minData = trainData.loc[ind]
print(minData.shape)

# now the majority
ind1 = trainData[trainData['approved']==1].index
print(len(ind1))

# Separate the majority class
majData = trainData.loc[ind1]
print(majData.shape)
majData.head()

majSample=majData.sample(n=len(ind),random_state=123)

print(majSample.shape)
majSample.head()

# Concatenating both data sets and then shuffling the data set
balData = pd.concat([minData,majSample],axis = 0)

# Shuffling the data set
from sklearn.utils import shuffle
balData = shuffle(balData)
balData.head()

X_train_bal = balData
y_train_bal = balData['approved']
del X_train_bal['approved']

hmdaModel.fit(X_train_bal, y_train_bal)

from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import matplotlib.pyplot as plt
# predict_proba predicts the probability and predict just predicts the category
# y_pred = dt.predict_proba(X_test)[:, 1]
y_pred = hmdaModel.predict(X_test)

conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
#
# Print the confusion matrix using Matplotlib
#
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

"""## bayesian-optimization"""

!pip install bayesian-optimization

from bayes_opt import BayesianOptimization
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from sklearn.model_selection import cross_val_score

# Define function to compute cross-validated accuracy
def xgb_cv(n_estimators, max_depth, gamma, min_child_weight, subsample):
    model = XGBClassifier(
        n_estimators=int(n_estimators),
        max_depth=int(max_depth),
        gamma=gamma,
        min_child_weight=min_child_weight,
        subsample=subsample,
        random_state=42,
    )
    return np.mean(cross_val_score(model, X_train_bal, y_train_bal, cv=2, scoring="precision"))

# Define function to optimize XGBoost classifier using Bayesian Optimization
def optimize_xgb():
    def xgb_crossval(n_estimators, max_depth, gamma, min_child_weight, subsample):
        return xgb_cv(n_estimators, max_depth, gamma, min_child_weight, subsample)

    optimizer = BayesianOptimization(
        f=xgb_crossval,
        pbounds={
            "n_estimators": (100, 500),
            "max_depth": (3, 10),
            "gamma": (0, 1),
            "min_child_weight": (1, 10),
            "subsample": (0.5, 1),
        },
        random_state=42,
    )
    optimizer.maximize(n_iter=10)
    return optimizer.max

# Find optimal hyperparameters using Bayesian Optimization
best_params = optimize_xgb()['params']
print("Best hyperparameters found by Bayesian Optimization:", best_params)

# Train the XGBoost classifier with the best hyperparameters
best_xgb = XGBClassifier(
    n_estimators=int(best_params["n_estimators"]),
    max_depth=int(best_params["max_depth"]),
    gamma=best_params["gamma"],
    min_child_weight=best_params["min_child_weight"],
    subsample=best_params["subsample"],
    random_state=42,
)

best_xgb.fit(X_train_bal, y_train_bal)

# Evaluate the model on the test set
y_pred = best_xgb.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, best_xgb.predict_proba(X_test)[:, 1])

print("Test set accuracy with best hyperparameters:", accuracy)
print("Test set F1-score with best hyperparameters:", f1)
print("Test set ROC-AUC score with best hyperparameters:", roc_auc)

from sklearn.metrics import precision_score
precision_score = precision_score(y_test, y_pred)
precision_score

print(classification_report(y_test, y_pred))

"""## Recurring Feature Elimination"""

# Import libraries
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt

# Perform recursive feature elimination with cross-validation
rfe = RFE(best_xgb, n_features_to_select=50, verbose=1)
X_rfe = rfe.fit_transform(X_train, y_train)

# Get the feature ranking and scores
feature_ranking = rfe.estimator_.feature_importances_
feature_scores = rfe.estimator_.feature_importances_

fr_array  = zip(X_train, feature_ranking)
list_of_tuples = list(zip(X_train.columns, feature_ranking))
df_rk = pd.DataFrame(list_of_tuples, columns = ['Feature', 'Importance'])

df_rk.sort_values(by='Importance')
df_imp[df_imp['Importance']> 0.008].sort_values(by='Importance').plot.barh(x='Feature')