# -*- coding: utf-8 -*-
"""MLE_FINALPROJECT_1_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OL1JwnOk1NkOjDArKV-meNEHnAL4kSCj

## Data Preprocessing
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
# %matplotlib inline

# Ignore Warnings
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

#Write out the versions of all packages to requirements.txt
!pip freeze >> requirements.txt

# Remove the restriction on Jupyter that limits the columns displayed (the ... in the middle)
pd.set_option("display.max_columns", None)
pd.set_option('display.max_rows', None)
# Docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.set_option.html#

# Pretty Display of variables.  for instance, you can call df.head() and df.tail() in the same cell and BOTH display w/o print
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

# to display nice model diagram
from sklearn import set_config
set_config(display='diagram')

# Python ≥3.5 is required
import sys
assert sys.version_info >= (3, 5)

# Scikit-Learn ≥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"

print("\n Numpy: " + np.__version__)
print("\n sklearn: " + sklearn.__version__)

"""### Structural Analysis"""

df_complete = pd.read_csv('https://raw.githubusercontent.com/mounicakakarla1211/MLETraining/main/chasehmda.csv',delimiter="|")
df_complete.head()

selected_columns = ['loan_amount','income','combined_loan_to_value_ratio','loan_term', 'property_value',
           'loan_type', 'loan_purpose','construction_method', 'occupancy_type',
           'purchaser_type','applicant_credit_scoring_model','co_applicant_credit_scoring_model',
           'debt_to_income_ratio','business_or_commercial_purpose','action_taken',
          'applicant_age','co_applicant_age','applicant_sex','co_applicant_sex','applicant_ethnicity_1','co_applicant_ethnicity_1','applicant_race_1','co_applicant_race_1',
          'state_code','interest_rate']
df = df_complete[selected_columns]
df.sample(5)

print("Rows and Columns: \n", df.shape, "\n")
print("General Information: \n", df.info(), "\n")

# Count how many times each data type is present in the dataset
pd.value_counts(df.dtypes)
# How many unique values per feature
df.nunique().to_frame()

# Pull descriptive statistics from your overall dataset
df.describe(include='all').T

"""#### Change data types to the correct data types"""

df["loan_type"]=df["loan_type"].apply(str)
df["loan_purpose"]=df["loan_purpose"].apply(str)
df["construction_method"]=df["construction_method"].apply(str)
df["occupancy_type"]=df["occupancy_type"].apply(str)
df["purchaser_type"]=df["purchaser_type"].apply(str)
df["applicant_credit_scoring_model"]=df["applicant_credit_scoring_model"].apply(str)
df["co_applicant_credit_scoring_model"]=df["co_applicant_credit_scoring_model"].apply(str)
df["debt_to_income_ratio"]=df["debt_to_income_ratio"].apply(str)
df["business_or_commercial_purpose"]=df["business_or_commercial_purpose"].apply(str)
df["applicant_age"]=df["applicant_age"].apply(str)
df["applicant_ethnicity_1"] = df["applicant_ethnicity_1"].apply(str)
df["co_applicant_ethnicity_1"] = df["co_applicant_ethnicity_1"].apply(str)
df["applicant_race_1"] = df["applicant_race_1"].apply(str)
df["co_applicant_race_1"] = df["co_applicant_race_1"].apply(str)
df["applicant_sex"] = df["applicant_sex"].apply(str)
df["co_applicant_sex"] = df["co_applicant_sex"].apply(str)
df["action_taken"]=df["action_taken"].apply(str)

df.shape

"""#### converting target action_taken to binary classfication
- 1 -- Loan originated
- 2 -- Application approved but not accepted
- 3 -- Application denied
- 4 -- Application withdrawn by applicant
- 5 -- File closed for incompleteness
- 6 -- Purchased loan
- 7 -- Preapproval request denied
- 8 -- Preapproval request approved but not accepted
1,2,6,8 - approved 1 remaining unapproved 0

"""

df['action_taken'].unique()

d = {'1': 1,'2':1,'6':1,'8':1}
df['approved'] = df['action_taken'].map(d).fillna(0)

df['approved'].unique()

df.drop(columns=['action_taken'],inplace=True)

"""#### Cleaning Data"""

# Basic Data Cleaning
df.columns = df.columns.str.lower().str.replace(' ', '_') # A

string_columns = list(df.dtypes[df.dtypes == 'object'].index) # B

for col in string_columns:
    df[col] = df[col].str.lower().str.replace('<', 'less') # C
    df[col] = df[col].str.lower().str.replace('>', 'greater') # C

"""### Quality Analysis

#### Duplicates
"""

# Duplicates in the Columns?
df.duplicated().sum()
#drop duplicates
df.drop_duplicates(keep='first',inplace=True)

"""#### MISSING VALUES"""

# MISSING VALUES
df.isna().sum()

plt.figure(figsize=(15, 8))
sns.set_style('whitegrid')

g = sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
g.set_xlabel('Column Number')
g.set_ylabel('Sample Number')

# Missing Values per Feature (Big Holes)
df.isna().mean().sort_values().plot(
    kind="bar", figsize=(15, 4),
    title="Percentage of missing values per feature",
    ylabel="Ratio of missing values per feature");

# drop any col that is more than 80% empty
df = df.dropna(thresh=df.shape[0] * 0.20,axis=1)
df.shape

# Make a decision... drop rows that are 20% or more empty (you set the threshhold)
df = df.dropna(thresh=df.shape[1] * 0.80, axis = 0).reset_index(drop=True)
df.shape

plt.figure(figsize=(15, 8))
sns.set_style('whitegrid')

g = sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
# g = sns.heatmap(df_X.loc[df_X.isnull().sum(1).sort_values(ascending=1).index].isnull(), cbar=False, cmap='viridis')
g.set_xlabel('Column Number')
g.set_ylabel('Sample Number')

df.isna().sum()

numerics = ['int16','int32','int64','float64']

df.select_dtypes(exclude=numerics).columns

df.select_dtypes(include=numerics).columns

# filling na values
df[df.select_dtypes(include=numerics).columns] = df.select_dtypes(include=numerics).fillna(0)
df[df.select_dtypes(exclude=numerics).columns] = df.select_dtypes(exclude=numerics).fillna('Missing')
df[df.select_dtypes(exclude=numerics).columns] = df.select_dtypes(exclude=numerics).replace('nan',"Missing")
df[df.select_dtypes(exclude=numerics).columns] = df.select_dtypes(exclude=numerics).replace('8888',"Missing")
df[df.select_dtypes(exclude=numerics).columns] = df.select_dtypes(exclude=numerics).replace('9999',"Missing")

df.drop(df[df['state_code'] == 'Missing'].index, inplace = True)

"""#### Outliers"""

#Distributions of data in each feature
df_num = df[['loan_amount','income','property_value']]
df_num.hist(bins=25, figsize=(20, 10),layout=(4, 4),edgecolor="black")
plt.tight_layout();

df = df[df['income'] != 0.0]
df = df[df['property_value'] != 0.0]
df['loanamount_log'] = np.log(df['loan_amount'])
df['income_log'] = np.log(df['income'])
df['property_value_log'] = np.log(df['property_value'])

df_num = df[['loanamount_log','income_log','property_value_log']]
df_num.hist(bins=25, figsize=(20, 10),layout=(4, 4),edgecolor="black")
plt.tight_layout();

"""#### Dropping log columns"""

df.drop(columns=['loan_amount','income','property_value'],inplace=True)

df.describe().T

"""### Content analysis

#### Exploratory Data Analysis (EDA)

##### Univariate Analysis
"""

sns.set(rc={'figure.figsize':(15,4)})
sns.countplot(x = 'approved',data=df,width=0.8)
plt.show()
df['approved'].value_counts(normalize=True)

sns.set(rc={'figure.figsize':(15,4)})
for col in df.select_dtypes(exclude=numerics).columns:
    print('\n======================{}==================='.format(col))
    print(sns.countplot(x = col,data=df,order = df[col].value_counts().index,width=0.8))
    plt.show()
    print(df[col].value_counts())
    print('\n')
    print(df[col].value_counts(normalize=True))

"""##### Bivariate analysis"""

sns.set(rc={'figure.figsize':(30,10)})
sns.boxplot(y="interest_rate", x="state_code", data=df);

sns.set(rc={'figure.figsize':(30,10)})
sns.boxplot(y="property_value_log", x="state_code", data=df);

sns.set(rc={'figure.figsize':(30,10)})
sns.boxplot(y="combined_loan_to_value_ratio", x="approved", data=df);

sns.set(rc={'figure.figsize':(30,10)})
sns.boxplot(y="loan_term", x="approved", data=df);

sns.set(rc={'figure.figsize':(30,10)})
sns.boxplot(y="loanamount_log", x="approved", data=df);

sns.set(rc={'figure.figsize':(30,10)})
sns.boxplot(y="income_log", x="approved", data=df);

sns.set(rc={'figure.figsize':(30,10)})
sns.boxplot(y="property_value_log", x="approved", data=df);

sns.set(rc={'figure.figsize':(30,10)})
sns.boxplot(x="loan_purpose", y="interest_rate", data=df);

sns.set(rc={'figure.figsize':(30,10)})
sns.boxplot(x="loan_type", y="interest_rate", data=df);

sns.set(rc={'figure.figsize':(30,10)})
sns.lineplot(y="property_value_log", x="loanamount_log", data=df);

ax = plt.axes()
ax.scatter(df["property_value_log"], df["loanamount_log"], alpha=0.5)
ax.set_ylabel('loanamount_log')
ax.set_xlabel('property_value_log')
plt.show()

sns.set(rc={'figure.figsize':(10,5)})

for col in df.select_dtypes(exclude=numerics).columns:
    print('\n======================{}==================='.format(col))
    print(sns.countplot(x = col,hue='approved',data=df))
    plt.show()

df.drop(columns=['interest_rate'],inplace=True)

"""#### Correlation"""

df.corr()

sns.set(rc={'figure.figsize':(30,10)})
sns.set_context("talk", font_scale=0.7)

numData = df[df.select_dtypes(include=numerics).columns]
sns.heatmap(numData.iloc[:,1:].corr(method='spearman'), cmap='rainbow_r', annot=True)

numData.drop("approved", axis=1).apply(lambda x: x.corr(df.approved,method='spearman')).sort_values(ascending=False)

# Evaluate but remember to consider multicollinearity

# Computes feature correlation
df_corr = df.corr(method="spearman") # pearson assumes a linear relationship... spearman does not

# Create labels for the correlation matrix
labels = np.where(np.abs(df_corr)>0.75, "S",
                  np.where(np.abs(df_corr)>0.5, "M",
                           np.where(np.abs(df_corr)>0.25, "W", "")))

# Plot correlation matrix
plt.figure(figsize=(15, 15))
sns.heatmap(df_corr, mask=np.eye(len(df_corr)), square=True,
            center=0, annot=labels, fmt='', linewidths=.5,
            cmap="vlag", cbar_kws={"shrink": 0.8});

#  Creates a mask to remove the diagonal and the upper triangle.
lower_triangle_mask = np.tril(np.ones(df_corr.shape), k=-1).astype("bool")

#  Stack all correlations, after applying the mask
df_corr_stacked = df_corr.where(lower_triangle_mask).stack().sort_values()

#  Showing the lowest and highest correlations in the correlation matrix
display(df_corr_stacked)

"""#### Sample Analysis"""

#Distributions of data in each feature
df.hist(bins=25, figsize=(15, 10), layout=(-1, 5), edgecolor="black")
plt.tight_layout();

"""### Feature Selection"""

df.drop(columns=['property_value_log'],inplace=True)

df["loan_term"].nunique()

#Patterns
cols_continuous = df.select_dtypes(include="number").nunique() >= 15

# Create a new dataframe which only contains the continuous features
df_continuous = df[cols_continuous[cols_continuous].index]
df_continuous.shape

sns.pairplot(df_continuous,height=1.5,plot_kws={"s": 2, "alpha": 0.2},corner=True,aspect=2);

train = df
train_labels = train['approved']
train = train.drop(columns = 'approved')

import pandas as pd
import numpy as np

import lightgbm as lgb

from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
import seaborn as sns

import gc

from sklearn.exceptions import NotFittedError

from itertools import chain

class FeatureSelector():
    """
    Class for performing feature selection for machine learning or data preprocessing.

    Implements five different methods

        1. Remove columns with a missing percentage greater than a specified threshold
        2. Remove columns with a single unique value
        3. Remove collinear variables with a correlation greater than a specified correlation coefficient
        4. Remove features with 0.0 feature importance from a gradient boosting machine (gbm)
        5. Remove features that do not contribute to a specified cumulative feature importance from the gbm

    Attributes
    --------

    record_missing : dataframe
        Records the fraction of missing values for features with missing fraction above threshold

    record_single_unique : dataframe
        Records the features that have a single unique value

    record_collinear : dataframe
        Records the pairs of collinear variables with a correlation coefficient above the threshold

    record_zero_importance : dataframe
        Records the zero importance features in the data according to the gbm

    record_low_importance : dataframe
        Records the lowest importance features not needed to reach the threshold of cumulative importance according to the gbm

    feature_importances : dataframe
        All the features importances from the gbm

    removal_ops : dict
        Dictionary of removal operations and associated features for removal identified

    Notes
    --------

        - All 5 operations can be run with the `identify_all` method.
        - Calculating the feature importances requires labels (a supervised learning task)
          for training the gradient boosting machine
        - For the feature importances, the dataframe is first one-hot encoded before training the gbm.

    """

    def __init__(self):

        # Dataframes recording information about features to remove
        self.record_missing = None
        self.record_single_unique = None
        self.record_collinear = None
        self.record_zero_importance = None
        self.record_low_importance = None

        self.feature_importances = None

        # Dictionary to hold removal operations
        self.removal_ops = {}



    def identify_missing(self, data, missing_threshold):
        """Find the features with a fraction of missing values above `missing_threshold`"""

        self.missing_threshold = missing_threshold

        # Calculate the fraction of missing in each column
        missing_series = data.isnull().sum() / data.shape[0]

        self.missing_stats = pd.DataFrame(missing_series).rename(columns = {'index': 'feature', 0: 'missing_fraction'})

        # Find the columns with a missing percentage above the threshold
        record_missing = pd.DataFrame(missing_series[missing_series > missing_threshold]).reset_index().rename(columns = {'index': 'feature', 0: 'missing_fraction'})

        to_drop = list(record_missing['feature'])

        self.record_missing = record_missing
        self.removal_ops['missing'] = to_drop

        print('%d features with greater than %0.2f missing values.\n' % (len(self.removal_ops['missing']), self.missing_threshold))

    def identify_single_unique(self, data):
        """Identifies features with only a single unique value. NaNs do not count as a unique value. """

        # Calculate the unique counts in each column
        unique_counts = data.nunique()

        self.unique_stats = pd.DataFrame(unique_counts).rename(columns = {'index': 'feature', 0: 'nunique'})

        # Find the columns with only one unique count
        record_single_unique = pd.DataFrame(unique_counts[unique_counts == 1]).reset_index().rename(columns = {'index': 'feature', 0: 'nunique'})

        to_drop = list(record_single_unique['feature'])

        self.record_single_unique = record_single_unique
        self.removal_ops['single_unique'] = to_drop

        print('%d features with a single unique value.\n' % len(self.removal_ops['single_unique']))

    def identify_collinear(self, data, correlation_threshold):
        """
        Finds collinear features based on the correlation coefficient between features.
        For each pair of features with a correlation coefficient greather than `correlation_threshold`,
        only one of the pair is identified for removal.

        Using code adapted from: https://gist.github.com/Swarchal/e29a3a1113403710b6850590641f046c

        Parameters
        --------

        data : dataframe
            Data observations in the rows and features in the columns

        correlation_threshold : float between 0 and 1
            Value of the Pearson correlation cofficient for identifying correlation features

        """

        self.correlation_threshold = correlation_threshold

        # Calculate the correlations between every column
        corr_matrix = data.corr()

        self.corr_matrix = corr_matrix

        # Extract the upper triangle of the correlation matrix
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))

        # Select the features with correlations above the threshold
        # Need to use the absolute value
        to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]

        # Dataframe to hold correlated pairs
        record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])

        # Iterate through the columns to drop
        for column in to_drop:

            # Find the correlated features
            corr_features = list(upper.index[upper[column].abs() > correlation_threshold])

            # Find the correlated values
            corr_values = list(upper[column][upper[column].abs() > correlation_threshold])
            drop_features = [column for _ in range(len(corr_features))]

            # Record the information (need a temp df for now)
            temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,
                                             'corr_feature': corr_features,
                                             'corr_value': corr_values})

            # Add to dataframe
            record_collinear = record_collinear.append(temp_df, ignore_index = True)


        self.record_collinear = record_collinear
        self.removal_ops['collinear'] = to_drop

        print('%d features with a correlation greater than %0.2f.\n' % (len(self.removal_ops['collinear']), self.correlation_threshold))

    def identify_zero_importance(self, features, labels, eval_metric, task='classification',
                                 n_iterations=10, early_stopping = True):
        """

        Identify the features with zero importance according to a gradient boosting machine.
        The gbm can be trained with early stopping using a validation set to prevent overfitting.
        The feature importances are averaged over n_iterations to reduce variance.

        Uses the LightGBM implementation (http://lightgbm.readthedocs.io/en/latest/index.html)

        Parameters
        --------
        features : dataframe
            Data for training the model with observations in the rows
            and features in the columns

        labels : array, shape = (1, )
            Array of labels for training the model. These can be either binary
            (if task is 'classification') or continuous (if task is 'regression')

        eval_metric : string
            Evaluation metric to use for the gradient boosting machine

        task : string, default = 'classification'
            The machine learning task, either 'classification' or 'regression'

        n_iterations : int, default = 10
            Number of iterations to train the gradient boosting machine

        early_stopping : boolean, default = True
            Whether or not to use early stopping with a validation set when training


        Notes
        --------

        - Features are one-hot encoded to handle the categorical variables before training.
        - The gbm is not optimized for any particular task and might need some hyperparameter tuning
        - Feature importances, including zero importance features, can change across runs

        """

        # One hot encoding
        features = pd.get_dummies(features)

        # Extract feature names
        feature_names = list(features.columns)

        # Convert to np array
        features = np.array(features)
        labels = np.array(labels).reshape((-1, ))

        # Empty array for feature importances
        feature_importance_values = np.zeros(len(feature_names))

        print('Training Gradient Boosting Model\n')

        # Iterate through each fold
        for _ in range(n_iterations):

            if task == 'classification':
                model = lgb.LGBMClassifier(n_estimators=1000, learning_rate = 0.05, verbose = -1)

            elif task == 'regression':
                model = lgb.LGBMRegressor(n_estimators=1000, learning_rate = 0.05, verbose = -1)

            else:
                raise ValueError('Task must be either "classification" or "regression"')

            # If training using early stopping need a validation set
            if early_stopping:

                train_features, valid_features, train_labels, valid_labels = train_test_split(features, labels, test_size = 0.15)

                # Train the model with early stopping
                model.fit(train_features, train_labels, eval_metric = eval_metric,
                          eval_set = [(valid_features, valid_labels)],
                          early_stopping_rounds = 100, verbose = -1)

                # Clean up memory
                gc.enable()
                del train_features, train_labels, valid_features, valid_labels
                gc.collect()

            else:
                model.fit(features, labels)

            # Record the feature importances
            feature_importance_values += model.feature_importances_ / n_iterations

        feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})

        # Sort features according to importance
        feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)

        # Normalize the feature importances to add up to one
        feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()
        feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])

        # Extract the features with zero importance
        record_zero_importance = feature_importances[feature_importances['importance'] == 0.0]

        to_drop = list(record_zero_importance['feature'])

        self.feature_importances = feature_importances
        self.record_zero_importance = record_zero_importance
        self.removal_ops['zero_importance'] = to_drop

        print('\n%d features with zero importance.\n' % len(self.removal_ops['zero_importance']))

    def identify_low_importance(self, cumulative_importance):
        """
        Finds the lowest importance features not needed to account for `cumulative_importance`
        of the feature importance from the gradient boosting machine. As an example, if cumulative
        importance is set to 0.95, this will retain only the most important features needed to
        reach 95% of the total feature importance. The identified features are those not needed.

        Parameters
        --------
        cumulative_importance : float between 0 and 1
            The fraction of cumulative importance to account for

        """

        self.cumulative_importance = cumulative_importance

        # The feature importances need to be calculated before running
        if self.feature_importances is None:
            raise NotFittedError('Feature importances have not yet been determined. Call the `identify_zero_importance` method` first.')

        # Make sure most important features are on top
        self.feature_importances = self.feature_importances.sort_values('cumulative_importance')

        # Identify the features not needed to reach the cumulative_importance
        record_low_importance = self.feature_importances[self.feature_importances['cumulative_importance'] > cumulative_importance]

        to_drop = list(record_low_importance['feature'])

        self.record_low_importance = record_low_importance
        self.removal_ops['low_importance'] = to_drop

        print('%d features that do not contribute to cumulative importance of %0.2f.\n' % (len(self.removal_ops['low_importance']), self.cumulative_importance))

    def identify_all(self, features, labels, selection_params):
        """
        Use all five of the methods to identify features to remove.

        Parameters
        --------

        features : dataframe
            Data for training the model with observations in the rows
            and features in the columns

        labels : array, shape = (1, )
            Array of labels for training the model. These can be either binary
            (if task is 'classification') or continuous (if task is 'regression')

        selection_params : dict
           Parameters to use in the five feature selection methhods.
           Params must contain the keys ['missing_threshold', 'correlation_threshold', 'eval_metric', 'task', 'cumulative_importance']

        """

        # Check for all required parameters
        for param in ['missing_threshold', 'correlation_threshold', 'eval_metric', 'task', 'cumulative_importance']:
            if param not in selection_params.keys():
                raise ValueError('%s is a required parameter for this method' % param)

        # Implement each of the five methods
        self.identify_missing(features, selection_params['missing_threshold'])
        self.identify_single_unique(features)
        self.identify_collinear(features, selection_params['correlation_threshold'])
        self.identify_zero_importance(features, labels, selection_params['eval_metric'], selection_params['task'])
        self.identify_low_importance(selection_params['cumulative_importance'])

        # Find the number of features identified to drop
        self.n_identified = len(set(list(chain(*list(self.removal_ops.values())))))
        print('%d total features out of %d identified for removal.\n' % (self.n_identified, pd.get_dummies(features).shape[1]))

    def check_identified(self):
        """Check the identified features before removal. Returns a set of the unique features identified."""

        all_identified = set(list(chain(*list(self.removal_ops.values()))))
        print('%d features identified for removal' % len(all_identified))

        return all_identified


    def remove(self, data, methods):
        """
        Remove the features from the data according to the specified methods.

        Parameters
        --------
            data : dataframe
                Dataframe with features to remove
            methods : 'all' or list of methods
                If methods == 'all', any methods that have identified features will be used
                Otherwise, only the specified methods will be used.
                Can be one of ['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance']

        Return
        --------
            data : dataframe
                Dataframe with identified features removed


        Notes
        --------
            - This first one-hot encodes the categorical variables in accordance with the gradient boosting machine.
            - Check the features that will be removed before transforming data!

        """


        features_to_drop = []

        data = pd.get_dummies(data)

        if methods == 'all':

            print('{} methods have been run'.format(list(self.removal_ops.keys())))

            # Find the unique features to drop
            features_to_drop = set(list(chain(*list(self.removal_ops.values()))))

        else:
            # Iterate through the specified methods
            for method in methods:
                # Check to make sure the method has been run
                if method not in self.removal_ops.keys():
                    raise NotFittedError('%s method has not been run' % method)

                # Append the features identified for removal
                else:
                    features_to_drop.append(self.removal_ops[method])

            # Find the unique features to drop
            features_to_drop = set(list(chain(*features_to_drop)))

        # Remove the features and return the data
        data = data.drop(columns = features_to_drop)
        self.removed_features = features_to_drop

        print('Removed %d features' % len(features_to_drop))
        return data

    def plot_missing2(self):
        """Histogram of missing fraction in each feature"""
        if self.record_missing is None:
            raise NotImplementedError("Missing values have not been calculated. Run `identify_missing`")

        self.reset_plot()
        self.missing_stats.plot.hist(color = 'red', edgecolor = 'k', figsize = (6, 4), fontsize = 14)
        plt.ylabel('Frequency', size = 18)
        plt.xlabel('Missing Fraction', size = 18); plt.title('Missing Fraction Histogram', size = 18);

    def plot_missing(self, threshold=0.0):
        """Histogram of missing fraction in each feature and print columns with missing values above threshold"""
        if self.record_missing is None:
            raise NotImplementedError("Missing values have not been calculated. Run `identify_missing`")

        # Extract the column names with missing fraction above threshold
        cols_above_threshold = self.missing_stats[self.missing_stats['missing_fraction'] > threshold].index.tolist()
        print(f"Columns with missing fraction above {threshold:.2f}: {cols_above_threshold}")

        # Plot the missing fraction histogram
        self.reset_plot()
        self.missing_stats.plot.bar(y='missing_fraction', color='red', edgecolor='k', figsize=(len(self.missing_stats), 4), fontsize=14)
        plt.xticks(rotation=90)
        plt.ylabel('Missing Fraction', size=18)
        plt.xlabel('Feature', size=18)
        plt.title('Missing Fraction Histogram', size=18);


    def plot_unique(self):
        """Histogram of number of unique values in each feature"""
        if self.record_single_unique is None:
            raise NotImplementedError('Unique values have not been calculated. Run `identify_single_unique`')

        self.reset_plot()
        self.unique_stats.plot.hist(edgecolor = 'k', figsize = (6, 4), fontsize = 14)
        plt.ylabel('Frequency', size = 18)
        plt.xlabel('Unique Values', size = 18); plt.title('Unique Values Histogram', size = 18);


    def plot_collinear(self):
        """
        Heatmap of the features with correlations above the correlated threshold in the data.

        Notes
        --------
            - Not all of the plotted correlations are above the threshold because this plots
            all the variables that have been idenfitied as having even one correlation above the threshold
            - The features on the x-axis are those that will be removed. The features on the y-axis
            are the correlated feature with those on the x-axis

        """

        if self.record_collinear is None:
            raise NotImplementedError('Collinear features have not been idenfitied. Run `identify_collinear`.')

        # Identify the correlations that were above the threshold
        corr_matrix_plot = self.corr_matrix.loc[list(set(self.record_collinear['corr_feature'])),
                                                list(set(self.record_collinear['drop_feature']))]

        # Set up the matplotlib figure
        f, ax = plt.subplots(figsize=(10, 8))

        # Generate a custom diverging colormap
        cmap = sns.diverging_palette(220, 10, as_cmap=True)

        # Draw the heatmap with the mask and correct aspect ratio
        sns.heatmap(corr_matrix_plot, cmap=cmap, center=0,
                    linewidths=.25, cbar_kws={"shrink": 0.6})

        ax.set_yticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[0]))])
        ax.set_yticklabels(list(corr_matrix_plot.index), size = int(160 / corr_matrix_plot.shape[0]));

        ax.set_xticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[1]))])
        ax.set_xticklabels(list(corr_matrix_plot.columns), size = int(160 / corr_matrix_plot.shape[1]));

        plt.xlabel('Features to Remove', size = 8); plt.ylabel('Correlated Feature', size = 8)
        plt.title("Correlations Above Threshold", size = 14)

    def plot_feature_importances(self, threshold = None):
        """
        Plots 15 most important features and the cumulative importance of features.
        If `threshold` is provided, prints the number of features needed to reach `threshold` cumulative importance.

        Parameters
        --------
        threshold : float, between 0 and 1 default = None
            Threshold for printing information about cumulative importances

        """

        if self.record_zero_importance is None:
            raise NotImplementedError('Feature importances have not been determined. Run `idenfity_zero_importance`')

        self.reset_plot()

        # Make a horizontal bar chart of feature importances
        plt.figure(figsize = (10, 6))
        ax = plt.subplot()

        # Need to reverse the index to plot most important on top
        ax.barh(list(reversed(list(self.feature_importances.index[:15]))),
                self.feature_importances['normalized_importance'].head(15),
                align = 'center', edgecolor = 'k')

        # Set the yticks and labels
        ax.set_yticks(list(reversed(list(self.feature_importances.index[:15]))))
        ax.set_yticklabels(self.feature_importances['feature'].head(15), size = 12)

        # Plot labeling
        plt.xlabel('Normalized Importance', size = 16); plt.title('Feature Importances', size = 18)
        plt.show()

        # Cumulative importance plot
        plt.figure(figsize = (6, 4))
        plt.plot(list(range(1, len(self.feature_importances) + 1)), self.feature_importances['cumulative_importance'], 'r-')
        plt.xlabel('Number of Features', size = 14); plt.ylabel('Cumulative Importance', size = 14);
        plt.title('Cumulative Feature Importance', size = 16);

        if threshold:

            # Index of minimum number of features needed for cumulative importance threshold
            importance_index = np.min(np.where(self.feature_importances['cumulative_importance'] > threshold))
            plt.vlines(x = importance_index + 1, ymin = 0, ymax = 1, linestyles='--', colors = 'blue')
            plt.show()

            print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))


    def reset_plot(self):
        plt.rcParams = plt.rcParamsDefault

fs = FeatureSelector()

fs.identify_collinear(train, 0.50)
fs.plot_collinear()

fs.identify_zero_importance(train, train_labels, eval_metric='auc')

fs.plot_feature_importances(threshold = 0.99)

fs.identify_all(train, train_labels, {'missing_threshold': 0.8, 'correlation_threshold': 0.8, 'eval_metric': 'auc','task': 'classification', 'cumulative_importance': 0.99})

features_identified = fs.check_identified()

list(features_identified)[:10]

train_removed = fs.remove(train, methods = 'all')

train_removed.head()

"""## Feature Importance MI"""

from sklearn.metrics import mutual_info_score
def calculate_mi(series):
    return mutual_info_score(series, df.approved)

df_mi = df[df.select_dtypes(exclude=numerics).columns].apply(calculate_mi)
df_mi = df_mi.sort_values(ascending=False).to_frame(name='MI')


display(df_mi.head())
display(df_mi.tail())

display(df_mi)

df_mi[df_mi['MI'] > 0.003].sort_values(by='MI').plot.barh(figsize=(10, 6))

"""## Feature Importance"""

df = df.sample(30000)

from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()
numerics = ['int16','int32','int64','float64']
catDF = df.select_dtypes(exclude=numerics)
numDF = df.select_dtypes(include=numerics)

numDF.head()
numDF.drop(columns='approved',inplace=True)

scaler.fit_transform(numDF.values)# Scale all numeric columns
numDF = pd.DataFrame(scaler.fit_transform(numDF.values),columns=numDF.columns,index=numDF.index)
numDF.head()

# Encode your cat data
catDF = pd.get_dummies(catDF,drop_first=True)

catDF.head()

X = pd.concat([numDF,catDF],axis=1)
print(X.shape)
#Prepare the Y variable
Y = df['approved']

print(Y.shape)
X.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=123)

"""### Recursive Feature Elimination with Logistic Regression"""

# Import libraries
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt

# Create a linear regression model
model = LinearRegression()

# Perform recursive feature elimination with cross-validation
rfe = RFE(model, n_features_to_select=1, verbose=1)
X_rfe = rfe.fit_transform(X_train, y_train)

# Get the feature ranking and scores
feature_ranking = rfe.ranking_
feature_scores = rfe.estimator_.coef_

fr_array  = zip(X_train.columns, feature_ranking)
list_of_tuples = list(zip(X_train.columns, feature_ranking))
df_rk = pd.DataFrame(list_of_tuples, columns = ['Feature', 'Ranking'])

df_rk.sort_values(by='Ranking')

"""### RandomForestClassifier"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(max_depth=5, random_state=42, n_estimators = 300).fit(X_train, y_train)

# This will give you the list of Hyperparameters of your model
rf.get_params()

rf.feature_importances_
# create a new DataFrame with feature importances and column names
feature_importances = pd.DataFrame({'feature': X.columns, 'importance': rf.feature_importances_})

# sort the features by importance
feature_importances = feature_importances.sort_values('importance', ascending=False)

# print the feature importances
print(feature_importances)

df_fi = feature_importances[feature_importances['importance'] >= 0.03]
df_fi

df_fi.sort_values(by='importance').plot.barh(x='feature',figsize=(10, 6))